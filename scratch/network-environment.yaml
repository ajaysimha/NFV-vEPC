#This file is an example of an environment file for defining the isolated
#networks and related parameters.
resource_registry:
  # Network Interface templates to use (these files must exist)
  OS::TripleO::Compute::Net::SoftwareConfig: /home/stack/templates/compute.yaml
  OS::TripleO::Controller::Net::SoftwareConfig: /home/stack/templates/controller.yaml

  # Add later after installing 1 controller + 1 compute
  OS::TripleO::CephStorage::Net::SoftwareConfig:/home/stack/templates/ceph-storage.yaml

  OS::TripleO::NodeUserData: /home/stack/templates/first-boot.yaml
  OS::TripleO::NodeExtraConfigPost: /home/stack/templates/post-install.yaml

parameter_defaults:
  # This section is where deployment-specific configuration is done
  # CIDR subnet mask length for provisioning network
  ControlPlaneSubnetCidr: '24'
  # Gateway router for the provisioning network (or Undercloud IP)
  ControlPlaneDefaultRoute: 192.0.2.4
  ControlPlaneIp: 192.0.2.4
  EC2MetadataIp: 192.0.2.4  # Generally the IP of the Undercloud
  # Customize the IP subnets to match the local environment
  InternalApiNetCidr: 172.17.0.0/24
  StorageNetCidr: 172.18.0.0/24
  StorageMgmtNetCidr: 172.19.0.0/24
  TenantNetCidr: 172.16.0.0/24
  ExternalNetCidr: 10.19.108.0/24
  # Customize the VLAN IDs to match the local environment
  InternalApiNetworkVlanID: 1020
  StorageNetworkVlanID: 1030
  StorageMgmtNetworkVlanID: 1040
  TenantNetworkVlanID: 1050
  ExternalNetworkVlanID: 100
  # Customize the IP ranges on each network to use for static IPs and VIPs
  InternalApiAllocationPools: [{'start': '172.17.0.10', 'end': '172.17.0.200'}]
  StorageAllocationPools: [{'start': '172.18.0.10', 'end': '172.18.0.200'}]
  StorageMgmtAllocationPools: [{'start': '172.19.0.10', 'end': '172.19.0.200'}]
  TenantAllocationPools: [{'start': '172.16.0.10', 'end': '172.16.0.200'}]
  # Leave room if the external network is also used for floating IPs
  ExternalAllocationPools: [{'start': '10.19.108.23', 'end': '10.19.108.50'}]
  ExternalNetworkVip: 10.19.108.51

# Customize the IP ranges on each network to use for static IPs and VIPs


  # Gateway router for the external network
  ExternalInterfaceDefaultRoute: 10.19.108.254


  # Uncomnent if using the Management Network (see network-management.yaml)
  # ManagementNetCidr: 10.0.1.0/24
  # ManagementAllocationPools: [{'start': '10.0.1.10', 'end', '10.0.1.50'}]
  # Use either this parameter or ControlPlaneDefaultRoute in the NIC templates
  # ManagementInterfaceDefaultRoute: 10.0.1.1
  # Define the DNS servers (maximum 2) for the overcloud nodes
  DnsServers: ["8.8.8.8","8.8.4.4"]

  # Set to empty string to enable multiple external networks or VLANs
  NeutronExternalNetworkBridge: "''"

  # Customize bonding options, e.g. "mode=4 lacp_rate=1 updelay=1000 miimon=100"
  # LinuxBondInterfaceOptions: "mode=802.3ad" <== Don't use this use next one specifying details to make it faster
  LinuxBondInterfaceOptions: "mode=4 lacp_rate=1 updelay=1000 miimon=100"
  DpdkBondInterfaceOvsOptions: "bond_mode=balance-tcp lacp=active"

  ComputeKernelArgs: "iommu=pt intel_iommu=on default_hugepagesz=1GB hugepagesz=1G hugepages=12"

 # Flavor of controller and compute nodes:
  OvercloudControlFlavor: control
  OvercloudComputeFlavor: compute
  OvercloudCephStorageFlavor: ceph-storage

 # Number of nodes to deploy.
  ControllerCount: 1
  ComputeCount: 2
  CephStorageCount: 1

  ##########################
  # OVS DPDK configuration #
  ##########################

  ## NeutronDpdkCoreList and NeutronDpdkMemoryChannels are REQUIRED settings.
  ## Attempting to deploy DPDK without appropriate values will cause deployment to fail or lead to unstable deployments.

  # List of cores to be used for DPDK Poll Mode Driver
  NeutronDpdkCoreList: "'2,3,14,15'"

  # NeutronDpdkSocketMemory
  NeutronDpdkSocketMemory: "'1024,1024'"

  # NeutronDpdkDriverType
  NeutronDpdkDriverType: "vfio-pci"


  # Number of memory channels to be used for DPDK
  NeutronDpdkMemoryChannels: "4"


  # Datapath type for ovs bridges
  NeutronDatapathType: "netdev"

  # The vhost-user socket directory for OVS
  NeutronVhostuserSocketDir: "/var/run/openvswitch"


  # Reserve the RAM for the host processes
  NovaReservedHostMemory: "4096"

  # Add a list or range of physical CPU cores to be reserved for virtual machine processes
  NovaVcpuPinSet: ['4-11','24-35','16-23','36-47']

  # List the most restrictive filters first to make the filtering process for the nodes more efficient
  NovaSchedulerDefaultFilters: "RamFilter,ComputeFilter,AvailabilityZoneFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,PciPassthroughFilter,NUMATopologyFilter"

  # Set the tunnel type for tenant network (for example, vxlan or gre). To disable the tunnel type parameter, set the value to ""
  NeutronTunnelTypes: ""


  # Set the tenant network type for OpenStack Networking. The options available are vlan or vxlan:
  NeutronNetworkType: 'vlan'

  # Set the Open vSwitch logical to physical bridge mappings:
  NeutronBridgeMappings: 'datacentre:br-isolated, dpdk:br-link'

  # Set the OpenStack Networking ML2 and Open vSwitch VLAN mapping range:
  NeutronNetworkVLANRanges: 'datacentre:4000:4070, dpdk:4071:4071'

  # Set a list or range of physical CPU cores to be tuned:
  HostCpusList: "'0,1,12,13'"


